[{"id":0,"href":"/Pipelines/Stages/","title":"Stages","parent":"Pipelines","content":"Pipeline Stages are logical groupings of Tasks to reflect the structure of the process, for example your process has a Build, Test and Release phase, the Pipeline Stages can be configured to reflect this.\n Pipeline Stages  Properties can be accessed from each stage and task using ${Stage.Task.Property} - for example, in the \u0026ldquo;Build\u0026rdquo; Stage the \u0026ldquo;Trigger Image Enumeration\u0026rdquo; task is a REST API call with a JSON response. The response property \u0026ldquo;id\u0026rdquo; is referenced using ${Build.Trigger Image Enumeration.output.responseBody.id}\n"},{"id":1,"href":"/Pipelines/Tasks/","title":"Tasks","parent":"Pipelines","content":"Code Stream Tasks are the basic units of a Pipeline, with different task types interacting with different systems.\nCommon configuration    Some configuration is common accross all task types\nPrecondition and Continue on failure    The precondition field can be used to determine if a Task should be executed - if the pre-condition evaluates to true the task will execute.\nFor example, if a task should only execute if a previous task completed successfully you can test the status of that task ${Stage0.task0.output.status} == \u0026quot;COMPLETED\u0026quot;\nIf Continue on failure is checked, the failure of this Task will not cause the entire Pipeline to fail. It can be combined with the precondition field to conditionally run subsequent tasks by checking the status of the failing task.  Common Task Configuration  Task Notifications    Task notifications are almost identical to Pipeline notifications except that they offer a specific events for the task (completes, is waiting, fails, starting).\n Task Notifications  Rollback    The Task Rollback setting allows you to configure a Pipeline that will be executed in the event that the Task fails. This can be used to clean up any artefacts generated by the Task execution that may be in an incomplete state. When you select the Rollback Pipeline you can configure the inputs for the selected pipeline, which would typically be variables from your parent (failing) Pipeline.\n Task Rollback  Available Task Types    More detailed information about each specific Task type is    Bamboo     CI     Condition     Custom     Kubernetes     Pipeline     Poll    PowerShell   REST   SSH   TFS    User Operation     VMware Cloud Template    vRealize Orchestrator    "},{"id":2,"href":"/Getting-Started/","title":"Getting Started","parent":"","content":""},{"id":3,"href":"/Getting-Started/GitOps/","title":"GitOps in Code Stream","parent":"Getting Started","content":"!\n"},{"id":4,"href":"/Dashboards/","title":"Dashboards","parent":"","content":"Code Stream users can view Dashboards to review historic data for any and all pipelines executions.\nIn addition to the automatically generated default dashboards, Custom Dashboards can be created by developers and administrators to view specific results by adding widgets from the menu to display statistics.\n   "},{"id":5,"href":"/Executions/","title":"Executions","parent":"","content":"In Executions, youâ€™ll find a detailed account of every pipeline execution that can be filtered by Pipeline, Status, Tag - or any other property. You can view at a glance which pipelines have failed, where and the error messages returned.\n Pipeline Execution Overview  Clicking into an Execution will give you a detailed view of the Pipeline Stages, Tasks, Configuration, Inputs, Task Inputs, Task Outputs, Task execution logs, and the output JSON object from the execution. The Output JSON view is very useful for identifying variables from one task to pass to another - see Variables in Pipelines   Pipeline Execution Detail  It\u0026rsquo;s also worth noting that from the Pipelines view you can access the previous 5 executions directly, by clicking on the icons\n Pipeline Executions  Nested Executions    Nested executions are Pipeline Executions that have been run as part of the Pipeline Task within a parent Pipeline. By default they\u0026rsquo;re hidden from view in the Executions pane, however if you add the Show: Nested Executions filter to the view, you can identify them by the Comments section, and they will be tagged with the parent Execution ID.\n Nested Executions with their Parent Execution  "},{"id":6,"href":"/User-Operations/","title":"User Operations","parent":"","content":"To provide more granular management and governance of the DevOps timeline, User Operations can be configured to manage the continuation of the pipeline execution. When a pipeline is configured with an approval task, the pipeline is paused and queued for user interaction. These users can monitor approval tasks here on the User Operations tab.\nUntil a preconfigured user approves or rejects the pipeline task, the pipeline remains stopped. If the required user does not address the User Operation in the time allowed, the pipeline will expire.\n"},{"id":7,"href":"/Pipelines/","title":"Pipelines","parent":"","content":"A Pipeline is the primary mechanism for sequencing all the tasks that need to be performed, and is composed of one or more Stage, with one or more tasks in each stage.\nGeneral Pipeline Settings    The pipeline settings allow you to set the pipeline name, concurrency, description, icon and tags.Being able to change the concurrency of the pipeline is often useful if you\u0026rsquo;re using shared resources that either don\u0026rsquo;t have the capcity to host multiple running executions, or there are resources that cannot be shared   Pipeline general settings  Pipeline configuration    When editing a Pipeline there are four tabs to configure:\nWorkspace  The workspace tab configures the environment in which the pipeline runs\n Host specifies a Docker endpoint on which CI tasks and Custom Integrations will execute Builder image URL configures the container image that will be used for CI tasks or Custom Integrations. You can specify using the just an official image (e.g. python), the image and a tag (e.g. python:3.10.0a6-alpine) or a full URL (e.g. projects.registry.vmware.com/antrea/prom-prometheus:v2.19.3)The container image can be almost any image but it needs to have wget or curl in order to download and install the Code Stream CI Agent, which is installed when the container is spun up.    Image Registry selects the Docker Registry endpoint to use to pull the Builder image - if the registry requires credentials to pull an image you can specify them as part of the endpoint and those will be used. Working directory is the directory within a container image that will be used when running commands (workingDir, by default) in a CI task - more often than not, you can leave this blank to default to /build Cache is accessible to each Pipeline run and can be used to cache files and folders that are common between Pipeline runs - for example if you download dependencies before building a Go project, those dependencies could be cached to speed up future executions of the same pipeline. Environment Variables can be used to pass environment variables to a container (similar to the -e VAR_NAME=\u0026quot;value\u0026quot; flag in the docker run command) CPU limit if a CI task requires significant resources, the container\u0026rsquo;s allocated CPU can be increased - it\u0026rsquo;s not often required Memory limit if a CI task requires significant resources, container\u0026rsquo;s allocated Memory can be increased - it\u0026rsquo;s not often required Git clone - if the pipeline is triggered by a Git webhook, CI tasks will automatically clone the Git repository. Note: You will need to configure the pipeline Inputs with the Git auto-inject parameters for this to work!    Input   Model  The Model tab is where you configure the Stages and Tasks of the pipeline - it\u0026rsquo;s where you spend most of your time when creating and editing pipelines.\n A Stage is an encapsulation mechanism for tasks and are used for grouping the individual task execution statuses and results. A Task performs individual actions based on its type and configuration. Tasks can deploy VMware Cloud Templates, and perform actions on configured endpoints, or more generic tasks such as prompting for user interations with User Operation, polling a 3rd party data source with the Poll task, or even perform a REST call.   Output  Outputs can be mapped to values produced by tasks in a pipeline and can be useful when you\u0026rsquo;re nesting pipelines using the Pipeline task to return the results to the parent pipeline.\n  Variables in Pipelines    Most configurable fields within a Pipeline can also use [Variables], references to Input parameters, the output of other tasks or general pipeline properties by using a reference.\nThese can be accessed using by typing $, which will bring up the auto-completion:\n Reference auto-completion  Referring back to previous tasks is done using a heirarchy that matches the structure of the pipeline, for example: ${Build Stage.Build Task.output.exports.variableName} would refer to the value of a variable called variableName that was exported from a task called Build Task in the stage Build Stage.\nTasks return their output as JSON, and it\u0026rsquo;s often useful to look at a previously executed task to find the correct path to an output variable - if you look at the Execution of a Pipeline and examine the task details, you can click \u0026ldquo;View Output JSON\u0026rdquo; and use the \u0026ldquo;Path finder\u0026rdquo; option to discover the correct path: Notifications    The notifications tab allows you to configure notifications for pipeline events (completion, waiting for user interaction, failure, cancellation, and starting) using either an Email endpoint, Jira endpoint, or by creating a Webhook with a POST, PUT or PATCH payload.\nEmail   Pipeline Email Notifications   Ticket   Pipeline Jira Notifications   Webhook   Pipeline Webhook Notifications    More        Stages     Tasks     Bamboo     CI     Condition     Custom     Kubernetes     Pipeline     Poll    PowerShell   REST   SSH   TFS    User Operation     VMware Cloud Template    vRealize Orchestrator      Reference     Creating and using pipelines in vRealize Automation Code Stream  "},{"id":8,"href":"/Custom-Integrations/","title":"Custom Integrations","parent":"","content":"Custom Integrations allow you write custom code in Python, Shell or NodeJS, and execute your code as a Custom Task in a Stage of a Pipeline. When the Custom Integration task is executed, it uses the Docker Host endpoint and Container Image configured for the parent Pipeline.\nCreating a Custom Integration    The Custom Integration is essentially a YAML file with four sections:\n Runtime - defines the runtime for the Custom Integration (nodejs, python2, python3, shell) Code - this is a multi-line scalar (any indented text after the | symbol should be interpreted as a multi-line scalar value) of the code to execute Input Properties - an array of input properties that are used in the execution of the Code Output Properties - an array of output properties that are returned by the execution of the Code  Input Properties    The available input types are documented in article linked the reference section below, one of the easiest ways to understand the different types is to create a new Custom Integration, select the runtime of your choice, then view the generated placeholder content.\nShell  echo $inputPropertyName   Python  from context import getInput # Import the getInput function from context.py myInput = getInput(\u0026#34;inputPropertyName\u0026#34;)   NodeJS  var context = require(\u0026#34;./context.js\u0026#34;) // Import the getInput function from context.js var myInput = context.getInput(\u0026#34;inputPropertyName\u0026#34;);    Output Properties    Output properties are used to return values from the Custom Integration task - currently only one type (label) is supported. To return a value it should be out to the Output Property\nShell  export outputPropertyName = \u0026#34;outputPropertyValue\u0026#34;   Python  from context import setOutput # Import the setOutput function from context.py setOutput(\u0026#34;outputPropertyName\u0026#34;, \u0026#34;outputPropertyValue\u0026#34;)   NodeJS  var context = require(\u0026#34;./context.js\u0026#34;) // Import the setOutput function from context.js context.setOutput(\u0026#34;outputPropertyName\u0026#34;, \u0026#34;outputPropertyValue\u0026#34;);    Versioning and Releasing    When you create a new Custom Integration it will be created as a Draft. In order to use the Custom Integration in a Pipeline Task you must release a version - this means that the released version can\u0026rsquo;t be changed and cause the Pipeline to fail. When you create a Custom Task type in a Pipeline Stage you can select which version you wish to use.\n Version and release a Custom Integration  Example Custom Integration - Create a Basic Authentication Header    The below example code takes a username and password input and returns a basicAuthHeader output for each of the runtimes - it\u0026rsquo;s a very simple use case that helps me when working with a REST API that only accepts a basic authentication headers.\nThe Container image you specify in the Pipeline Workspace configuration must support the runtime you\u0026rsquo;re using for the Custom Integration code  Shell  ---runtime:shellcode:| export basicAuthHeader=$(echo -n $username:$password | base64)inputProperties:# Username input- name:\u0026#39;username\u0026#39;type:texttitle:\u0026#39;Username\u0026#39;placeHolder:\u0026#39;Enter basic authentication usename\u0026#39;required:truebindable:true# Password input- name:\u0026#39;password\u0026#39;type:passwordtitle:\u0026#39;Password\u0026#39;placeHolder:\u0026#39;Enter basic authentication password\u0026#39;defaultValue:\u0026#39;\u0026#39;required:truebindable:trueoutputProperties:- name:basicAuthHeadertype:labeltitle:BasicAuthenticationHeader```  Python  ---runtime:\u0026#34;python3\u0026#34;code:| from base64 import b64encodefromcontextimportgetInput,setOutputusername=getInput(\u0026#39;username\u0026#39;)password=getInput(\u0026#39;password\u0026#39;)usernameAndPassword=b64encode(bytes(f\u0026#39;{username}:{password}\u0026#39;,encoding=\u0026#39;ascii\u0026#39;)).decode(\u0026#39;ascii\u0026#39;)setOutput(\u0026#39;basicAuthHeader\u0026#39;,\u0026#34;Basic \u0026#34;+usernameAndPassword)inputProperties:# Username input- name:\u0026#39;username\u0026#39;type:texttitle:\u0026#39;Username\u0026#39;placeHolder:\u0026#39;Enter basic authentication usename\u0026#39;required:truebindable:true# Password input- name:\u0026#39;password\u0026#39;type:passwordtitle:\u0026#39;Password\u0026#39;placeHolder:\u0026#39;Enter basic authentication password\u0026#39;defaultValue:\u0026#39;\u0026#39;required:truebindable:trueoutputProperties:- name:basicAuthHeadertype:labeltitle:BasicAuthenticationHeader  NodeJS  ---runtime:\u0026#34;nodejs\u0026#34;code:| var context = require(\u0026#34;./context.js\u0026#34;)varusername=context.getInput(\u0026#34;username\u0026#34;);;varpassword=context.getInput(\u0026#34;password\u0026#34;);;constbuff=Buffer.from(username+\u0026#39;:\u0026#39;+password,\u0026#39;utf-8\u0026#39;);constbase64=buff.toString(\u0026#39;base64\u0026#39;);context.setOutput(\u0026#34;basicAuthHeader\u0026#34;,base64);inputProperties:# Username input- name:\u0026#39;username\u0026#39;type:texttitle:\u0026#39;Username\u0026#39;placeHolder:\u0026#39;Enter basic authentication usename\u0026#39;required:truebindable:true# Password input- name:\u0026#39;password\u0026#39;type:passwordtitle:\u0026#39;Password\u0026#39;placeHolder:\u0026#39;Enter basic authentication password\u0026#39;defaultValue:\u0026#39;\u0026#39;required:truebindable:trueoutputProperties:- name:basicAuthHeadertype:labeltitle:BasicAuthenticationHeader   To use my new task in a Pipeline stage, I create a new Task, select Type Custom and select my Custom Integration and released version. This automatically creates an input form for the username and password input properties. Because I have set bindable: true for these properties, I can bind them to Pipeline variables, or Variables\n Using a Custom Integration in a Custom task  To access the output property basicAuthHeader later on in my REST task, I can access the task properties by using the Stage name, Task name, then output.properties.propertyName - e.g:\n${Authenticate.Create Auth Header.output.properties.basicAuthHeader}\nReference     How do I integrate my own build, test, and deploy tools with vRealize Automation Code Stream  "},{"id":9,"href":"/Configure/Projects/","title":"Projects","parent":"Configure","content":"A Project is where you combine resources, with users, to put all your Pipelines, variables etc. in Codestream, to share between the members of the project.\n"},{"id":10,"href":"/Configure/Endpoints/","title":"Endpoints","parent":"Configure","content":"Configuring unique and separate Endpoints across multiple platforms and isolated environments allows for the integration, automation, and management of a complete software delivery solution for the entire DevOps timeline from beginning to end.\nEndpoints allow VMware Cloud Services to connect to remote applications and data sources. Code Stream becomes the focal point in the DevOps release process as a single unifying platform integration tool, acting as the glue between established industry standard DevOps toolsets.\n"},{"id":11,"href":"/Configure/variables/","title":"Variables","parent":"Configure","content":"Variables are a great way to keep text, secrets etc. that you have to reuse, in your Pipelines, in one central place, to easily manage. It\u0026rsquo;s also a good practice, if you need to export your pipelines, to make sure, that sensetive information is not exported as well.\nCreate      Create variables  Type     Regular - Value is not hidden Secret - Value is hidden Restricted - Pipelines containing Restricted variables, can only be run by administrators  Project    Variables need to be attached to a project.\nNames    The name you use, when you reference the variable\nValue    The actual value of the variable\nDescription    The description of the value, to easely identify the variable and the use of it.\nUse    When using the variable, all you need to do, is to write ${var.NameOfVariable} where you want to use it.\nSo a variable with the name of mysecret will look like\n${var.mysecret} in your pipeline\n"},{"id":12,"href":"/Configure/","title":"Configure","parent":"","content":""},{"id":13,"href":"/Triggers/Gerrit/","title":"Gerrit","parent":"Triggers","content":""},{"id":14,"href":"/Triggers/Git/","title":"Git","parent":"Triggers","content":""},{"id":15,"href":"/Triggers/Docker/","title":"Docker","parent":"Triggers","content":""},{"id":16,"href":"/Triggers/","title":"Triggers","parent":"","content":"Triggers are a way for Code Stream to integrate with Docker, Gerrit, and Git lifecycles. Code Stream connects to the respective endpoint through a Webhook.\nA Webhook is configured by an administrator for a push or pull request event on the Triggers tab. Through the webhook, any code change events on the remote repository are received by the trigger in Code Stream.\nActivity for Docker, Gerrit, and Git triggers and their webhooks can also be observed on the Triggers tab.\n"},{"id":17,"href":"/","title":"","parent":"","content":"What is Code Stream?    Code Stream is a service provided as part of VMware vRealize Automation, either as SaaS through VMware Cloud Services or an on-premises deployment.\nCode Stream is a continuous integration and delivery (CI/CD) release pipeline tool that allows developers to model and automate the entire release process. It incorporates a release dashboard to keep track of all the various release KPIs and acts as the glue between all existing DevOps tools in the release process.\nCode Stream can help teams to deliver software and code changes faster, more reliably and with higher quality while reducing manual operations and operational risk traditionally associated with releases.\nCode Stream provides a customizable dashboards so that DevOps teams can measure their release KPIs and identify bottlenecks or problem areas in the release process.\nCode Stream can be extended using the Custom Integrations feature to interact with almost any 3rd party system that has an API or CLI.\nIf you want to find out more about vRealize Automation you can visit the official product site, view product features, or take a closer look at Code Stream. You can alse explore vRealize Automation Cloud with a free 45-day trial.\n"},{"id":18,"href":"/Configure/Endpoints/bamboo/","title":"Bamboo","parent":"Endpoints","content":"  Bamboo  The Atlassian Bamboo endpoint is easy to configure.\nJust type in\n Your Project The name of the Bamboo endpoint Cloud proxy (If using VRA Cloud) Url Username Password  "},{"id":19,"href":"/Pipelines/Tasks/bamboo/","title":"Bamboo","parent":"Tasks","content":"Bamboo enables you to integrate to a Atlassisan Bamboo server\nAs a precondition, you have to have setup your Bamboo endpoint,\nYou can select teh Endpoint, the Project and the Plan of your Banboo server, and run it directly in your pipeline.\n Bamboo  "},{"id":20,"href":"/categories/","title":"Categories","parent":"","content":""},{"id":21,"href":"/Pipelines/Tasks/CI/","title":"CI","parent":"Tasks","content":"The CI task enables almost any action in your pipeline by pulling a specific Docker image from a registry endpoint, and deploying it to a Docker host configured as an Endpoint. It then executes the CI task script in the context of the running container. It is an incredibly powerful and flexible task type, because the image can have almost any tool or program in it.\nThe CI task runs using parameters configured in the Pipeline Workspace configuration, including the Container Image, Docker Registry, Docker Host, directory, cache, environment variables and CPU/Memory limits.\nThe CI task container will run for the lifespan of the entire pipeline, so subsequent CI tasks within a pipeline will be executed in the same environment - this means that you can, for example, write files in container that will be preserved between Tasks.\nCreating a CI task    As well as the common configurations available to all tasks, the CI task has:\n Steps - the script to execute in the container Preserve artifacts - the paths of artifacts to preserve in the shared path (on the Docker host) - e.g. a compiled binary, configuration file or SSH key Export - exported variable names to be made available to the Pipeline after the task executes Process configurations - configuration elements for JUnit, JaCoCo, Checkstyle, FindBugs processing  Steps    The steps stage is essentially a Shell script that will be executed on the CI container image. This means it\u0026rsquo;s relatively easy to convert any exisitng Shell scripts into a repeatable CI task by using Pipeline Inputs, Variables, and Pipeline Variable Binding.\n An example CI Task steps  Preserve Artifacts    Artifacts (files) specified here will be stored in a Shared Folder on the Docker host configured in the Pipeline workspace. The full path of the preserved file will be available in the Task output JSON for later use. If, for example, my CI task writes to a file \u0026ldquo;preserve.txt\u0026rdquo; and specifies the file name/path in the Perserve Artifacts setting, the full path to \u0026ldquo;preserve.txt\u0026rdquo; will be available in ${Stage0.Task0.output.artifacts[0]}\n Preserved Artifact Path in Output JSON  Exports    You can use the Exports feature when you want the result of a CI task to be available to the Pipeline. These are simple string exports that must be compatible with the JSON output format that the task returns. Exporting multiline strings, for example, does not export correctly.\nAn example use might be a BUILDTIME variable that will be used in naming throughout the pipeline - in the CI task steps the variable must be declared with the export keyword:\nexport BUILDTIME=\u0026#34;$(date \u0026#34;+%Y%m%d-%H%M%S\u0026#34;)\u0026#34; Then the BUILDTIME variable can be added to the Export list and accessed later using Pipeline Variable Binding\n Export BUILDTIME variable from a CI task  References     What variables and expressions can I use when binding pipeline tasks in VMware Code Stream  "},{"id":22,"href":"/Pipelines/Tasks/Condition/","title":"Condition","parent":"Tasks","content":"The Condition Task is very similar to the Precondition setting available for all Tasks, without having an additional task attached to it. It can be used to evaluate the success of previous Stages before moving on with the Pipeline, or to trigger the failure of a pipeline based on a set of conditions. Simple operators can be used to compare Pipeline task output, Variables, Inputs, or any other property that is accessible in the Pipeline.\n Precondition  An example could be, that you have an input value, in your pipeline, with the name of \u0026ldquo;run\u0026rdquo;. In your task, you put in the Precondition of ${input.run} == \u0026quot;true\u0026quot; which means that this task will only run if the value if run is true\n"},{"id":23,"href":"/Pipelines/Tasks/Custom/","title":"Custom","parent":"Tasks","content":"The Custom Task allows you to use Custom Integrations in Pipeline Stages - for more details please see the Custom Integrations page.\n Using a Custom Integration in a Custom Task  "},{"id":24,"href":"/Configure/Endpoints/docker/","title":"Docker","parent":"Endpoints","content":"ggg\n"},{"id":25,"href":"/Configure/Endpoints/dockeregistry/","title":"Docker Registry","parent":"Endpoints","content":"ggg\n"},{"id":26,"href":"/Configure/Endpoints/email/","title":"Email","parent":"Endpoints","content":"ggg\n"},{"id":27,"href":"/Configure/Endpoints/gerrit/","title":"Gerrit","parent":"Endpoints","content":"ggg\n"},{"id":28,"href":"/Configure/Endpoints/git/","title":"Git","parent":"Endpoints","content":"ggg\n"},{"id":29,"href":"/Getting-Started/Hello-World/","title":"Hello World","parent":"Getting Started","content":"My first pipeline\n"},{"id":30,"href":"/Configure/Endpoints/jenkins/","title":"Jenkins","parent":"Endpoints","content":"ggg\n"},{"id":31,"href":"/Configure/Endpoints/jira/","title":"Jira","parent":"Endpoints","content":"ggg\n"},{"id":32,"href":"/Configure/Endpoints/kubernetes/","title":"Kubernetes","parent":"Endpoints","content":"ggg\n"},{"id":33,"href":"/Pipelines/Tasks/Kubernetes/","title":"Kubernetes","parent":"Tasks","content":"The Kubernetes Task allows for interation and execution of Get, Create, Apply, Delete and Rollback actions against a Kubernetes Endpoint. You can use a Git Endpoint for the YAML manifest definitions. The combination of Git Endpoints, use of a Git Triggers and the Kubernetes task allows you to build complex Kubernetes application deployment strategies.\nCommon across all the Kubernetes actions is are:\n Kubernetes Cluster - the Kubernetes endpoint that the task will execute against Timeout - the task timeout while waiting for a response from Kubernetes, this can be useful if, for example, your deployment task needs to download several large container images that may exceed the default timeout, or your deployment has readiness checks that take time to return OK. Source Type  Local definition uses the YAML entered in the Local YAML Definition field - this allows you to use variables, properties and inputs directly in the YAML definition to customise the YAML Source Control uses the YAML loaded from a Git repository specified as a Git Endpoint, which can be customised by using variables in the YAML source file and parameters in the task configuration. e.g. by creating a YAML template with the $${NAMESPACE} variable below, the namespace name can be customised with a parameter: apiVersion:v1kind:Namespacemetadata:name:$${NAMESPACE}  YAML from Source Control, with Parameters      Actions    There are five different actions you can perform against the Kubernetes endpoint.\nGet  The Get action allows us to retrieve objects from Kubernetes based on a YAML specification - the minimum we need to specify is the object type, the name of the object and the namespace that the object resides, for example the query kubectl get deployment controller --namespace metallb-system translates to the following YAML:\napiVersion:apps/v1kind:Deploymentmetadata:name:controllernamespace:metallb-system Kubernetes Get Task  When the Kubernetes task runs the returned JSON object is accessible as part of the Task response object. This means that you can access properties of the requested object in subsequent tasks - e.g. to examine the number of deployments in the replica spec, I could access ${Stage0.Kubernetes.output.response.deployments.controller.spec.replicas}\n Kubernetes Get Task Response   Create  The Create action allows us to create objects based on a YAML specification, and as is common accross all tasks you can use locally defined or Git sourced YAML definitions. There is also an option to Continue on conflict - if enabled, and the object you are trying to create already exists, then the Task will not fail the Pipeline but continue on to the next Task or Stage. This can be useful if you want to ensure an object exists but you are not sure if it already does - a common example would be creating a namespace.\nYou can also use YAML files with multiple documents, separated with --- to conform with YAML specifications - for example, the definition below creates two namespace:\nCreating all the kubernetes resources with a single manifest file may be appropriate in some cases, however any failure of a single object creation will result in the whole task failing. If you need to respond to specific failures, or for ease of troubleshooting, it\u0026rsquo;s normally a better idea to split the objects into multiple tasks and manifests.   Create multiple Kubernetes Namespaces  When this Create task is executed, the response JSON will include responses for each object creation (or associated failure)\n Response for multiple Kubernetes Namespaces   Apply  The Apply action allows us to modify existing objects by applying an updated manifest YAML - this can be useful in a GitOps scenario when we are using Git as the single source of truth for an application. In this case when a YAML file is updated, a Git Trigger will start the pipeline and apply the updated YAML manifest to the existing object.\nIf we take the namespace from in the Create task, ns-01, and want to update the metadata with a label we can apply an updated manifest:\napiVersion:v1kind:Namespacemetadata:name:ns-01labels:app:learn-code-streamWhen this task executes, the response shows that the existing object has been updated with the new label:\n Updated namespace includes the label   Delete  The Delete action allows us to delete objects from Kubernetes based on a YAML specification - as with the Get action we need to specify enough information to identify the object we wish to delete. Most often, this is the object type, the name of the object and the namespace that the object resides - however to delete the namespace we created in the previous task we would use kubectl delete namespace ns-01, which translates to the following YAML:\napiVersion:v1kind:Namespacemetadata:name:ns-01When executed, the task succeeds if the object has been removed and fails if the deletion fails. The JSON output will reflect the new status:\n Successfully deleted   Rollback  The Rollback action relates to the management of deployment, statefulset and daemonset objects in kubernetes. Whenever the Pod template (.spec.template) properties for these objects are modified a rollout is triggered and a record of the previous state is recorded so that you can roll back to a previous state if required. The Rollback action allows you trigger a rollback to one of the previous five states and would typically be used to recover from a failed update to an existing deployment, statefulset or daemonset.\n Rollback the controller deployment to the previous version    References     Rolling Back a Deployment  "},{"id":34,"href":"/Pipelines/Tasks/Pipeline/","title":"Pipeline","parent":"Tasks","content":"The Pipeline task allows you to nest existing Pipelines within a parent pipeline, which is really useful for chaining together smaller units of work within a larger parent process. The Pipeline task will automatically generate fields for the Inputs of the nested Pipeline, and the Output parameters will be available to the parent Pipeline as the output properties of the Pipeline task.\nThe Task configured below will execute a Pipeline called \u0026ldquo;vra-POST\u0026rdquo;, the three Input parameters (vraAccessToken, vraRequestPayload, vraRequestUri) for the Pipeline have been automatically added to the task and the one output paramter vraResponseJSON has been added to the output parameters.\n Nested Pipeline Configuration  When viewing a parent Pipeline Execution, nested Pipeline tasks will have an icon to indicate they are nested. When you examine the Task in the Execution, there is a link to the nested Pipeline execution in the Result section. You can also details of the nested Pipeline in the Pipeline Output section.\n Nested Pipeline Execution  "},{"id":35,"href":"/Pipelines/Tasks/Poll/","title":"Poll","parent":"Tasks","content":"Poll\n"},{"id":36,"href":"/Pipelines/Tasks/PowerShell/","title":"PowerShell","parent":"Tasks","content":""},{"id":37,"href":"/Getting-Started/Publishing-Pipelines/","title":"Publishing Pipelines","parent":"Getting Started","content":"Code Stream Pipelines can be published to Service Broker with a Custom Form, allowing you to create a much more interactive user experiece.\n"},{"id":38,"href":"/Pipelines/Tasks/REST/","title":"REST","parent":"Tasks","content":""},{"id":39,"href":"/Pipelines/Tasks/SSH/","title":"SSH","parent":"Tasks","content":""},{"id":40,"href":"/tags/","title":"Tags","parent":"","content":""},{"id":41,"href":"/Configure/Endpoints/tfs/","title":"TFS","parent":"Endpoints","content":"ggg\n"},{"id":42,"href":"/Pipelines/Tasks/TFS/","title":"TFS","parent":"Tasks","content":""},{"id":43,"href":"/Pipelines/Tasks/User-Operation/","title":"User Operation","parent":"Tasks","content":"User Operation is a way to pause a running pipeline, to ask a user for approval.\nApprovers kan be both a list of users or groups.\nA custom description can be created, and send by email.\nA timeout for approvals can also be set. When the time expires, the approval will be denied.  User Operations  "},{"id":44,"href":"/Pipelines/Tasks/Cloud-Template/","title":"VMware Cloud Template","parent":"Tasks","content":"Hmmm\n"},{"id":45,"href":"/Pipelines/Tasks/vRealize-Orchestrator/","title":"vRealize Orchestrator","parent":"Tasks","content":""},{"id":46,"href":"/Configure/Endpoints/vro/","title":"vRealize Orchestrator (vRO)","parent":"Endpoints","content":"ggg\n"}]